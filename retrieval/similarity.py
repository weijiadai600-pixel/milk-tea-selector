# retrieval/similarity.py
# ------------------------------------------------------------
# Purpose: Generic utilities for "embedding loading + similarity computation + Top-K retrieval"
# ✅ Newly added (key idea of approach 1):
#   1) Use md5(hash) to determine whether the query image is exactly one image in the database
#   2) If so, exclude it during retrieval (avoid top1 always being itself)
# ------------------------------------------------------------

import os
import hashlib
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn.functional as F


def load_embeddings(embedding_path: str):
    """
    Load saved milk tea image embeddings (generated by scripts/extract_features.py)

    Returns:
    - embeddings: Tensor [N, D]
    - paths: List[str]   (relative paths or saved paths for each image)
    - labels: List[str]  (brand names)
    """
    # map_location="cpu": ensures it can be loaded even without a GPU
    data = torch.load(embedding_path, map_location="cpu", weights_only=False)

    embeddings = data["features"]  # [N, D]
    paths = data["paths"]         # List[str]
    labels = data["labels"]       # List[str]
    return embeddings, paths, labels


# ============================================================
# ✅[Added] 1) md5 helper: compute file-content hash for an image
# ============================================================
def md5_file(file_path: str, chunk_size: int = 1024 * 1024) -> Optional[str]:
    """
    Compute md5 of file contents to determine whether two files are the exact same image.

    Why do we need this?
    - The uploaded/query image can be from any local path;
    - The database image path might be different (or the file might have been copied/renamed);
    - md5(hash) provides a stable way to check if the content is identical.

    Returns:
    - md5 hex string; returns None if the file does not exist or cannot be read
    """
    if not os.path.exists(file_path):
        return None

    h = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            while True:
                b = f.read(chunk_size)
                if not b:
                    break
                h.update(b)
        return h.hexdigest()
    except Exception:
        return None


# ============================================================
# ✅[Added] 2) Build a hash index for the database: md5 -> index
# ============================================================
def build_hash_index(
    paths: List[str],
    project_root: str,
) -> Dict[str, int]:
    """
    Build a md5 -> index mapping for every image in the database.

    Args:
    - paths: image paths stored in the embeddings file (typically like data/images/brand/xxx.jpg)
    - project_root: project root directory (final_project), used to build absolute image paths

    Returns:
    - hash2idx: dict with key=md5 hex string, value=index of that image in embeddings

    Notes:
    - This reads all image files, so it can take time for large datasets;
    - In Streamlit, you should cache it (app.py does that) to avoid recomputing on every rerun.
    """
    hash2idx: Dict[str, int] = {}

    for i, p in enumerate(paths):
        abs_path = os.path.join(project_root, p)
        md5 = md5_file(abs_path)
        if md5 is None:
            # Skip missing/unreadable files to avoid crashing
            continue
        hash2idx[md5] = i

    return hash2idx


# ============================================================
# ✅[Added] 3) Find the query's index in DB via "path first + hash fallback"
# ============================================================
def find_self_index_by_hash_or_path(
    query_path: str,
    paths: List[str],
    project_root: str,
    hash2idx: Optional[Dict[str, int]] = None,
) -> Optional[int]:
    """
    Try to determine whether the query image belongs to the database.

    Matching strategy (fast and robust):
    1) Path matching first:
       - If query_path is already like data/images/...,
         normalize it and directly search in `paths`.
       - Fastest: no hash computation needed.
    2) If path matching fails, fall back to hash matching:
       - Compute md5 for the query file and look it up in `hash2idx`.
       - Robust even if the query image was copied/renamed.

    Returns:
    - If query belongs to the database, return its index in embeddings;
    - Otherwise return None (meaning query is not in DB, no need to exclude top1).
    """
    # ---------- 1) Path matching ----------
    # Normalize query_path into a path relative to project_root for comparison.
    # Example:
    #   query_path = "/home/dwj/.../final_project/data/images/a/b.jpg"
    #   -> rel_query = "data/images/a/b.jpg"
    try:
        abs_query = os.path.abspath(query_path)
        abs_root = os.path.abspath(project_root)
        if abs_query.startswith(abs_root):
            rel_query = os.path.relpath(abs_query, abs_root).replace("\\", "/")
            if rel_query in paths:
                return paths.index(rel_query)
    except Exception:
        pass

    # ---------- 2) Hash matching ----------
    if hash2idx is None:
        # If no hash2idx is provided, build it on the fly (OK for CLI; Streamlit should pass cached one)
        hash2idx = build_hash_index(paths, project_root)

    q_md5 = md5_file(query_path)
    if q_md5 is None:
        return None

    return hash2idx.get(q_md5, None)


# ============================================================
# Cosine similarity
# ============================================================
def cosine_similarity(query_feat: torch.Tensor, gallery_feats: torch.Tensor) -> torch.Tensor:
    """
    Compute cosine similarity between query and all gallery images.
    - query_feat: [D]
    - gallery_feats: [N, D]

    Returns:
    - sims: [N]
    """
    query_feat = F.normalize(query_feat, dim=0)
    gallery_feats = F.normalize(gallery_feats, dim=1)
    sims = torch.matmul(gallery_feats, query_feat)  # [N]
    return sims


def retrieve_top_k(
    query_feat: torch.Tensor,
    embeddings: torch.Tensor,
    paths: List[str],
    labels: List[str],
    top_k: int = 5,
    exclude_index: Optional[int] = None,
):
    """
    Given query_feat, retrieve the most similar top_k images.

    ✅[Update] Add `exclude_index`:
    - Used to exclude the query image itself (avoid top1 always being the same image)

    Args:
    - exclude_index: if not None, set the similarity at that index to a very small value
      so it will never be selected.
    """
    sims = cosine_similarity(query_feat, embeddings)

    # ✅ Key: exclude "self"
    if exclude_index is not None and 0 <= exclude_index < sims.numel():
        sims[exclude_index] = -1e9

    topk_vals, topk_idx = torch.topk(sims, k=min(top_k, sims.numel()))

    results = []
    for score, idx in zip(topk_vals.tolist(), topk_idx.tolist()):
        results.append({
            "path": paths[idx],
            "label": labels[idx],
            "score": float(score),
            "index": idx,  # keep index (useful for UI selection later)
        })
    return results


if __name__ == "__main__":
    # This main is only for a quick self-test
    embedding_path = "data/embeddings/milktea_vit_embeddings.pt"
    embeddings, paths, labels = load_embeddings(embedding_path)

    print(f"Loaded {len(paths)} milk tea images in total")
